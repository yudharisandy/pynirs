{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aq5dbT2kZq0L"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "by Yudha Putra Arisandy\n",
        "Departement of Computer Science\n",
        "IPB University\n",
        "Indonesia\n",
        "\"\"\"\n",
        "\n",
        "# Import library\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Read dataframe for excel file\n",
        "df = pd.read_excel('Data Pre-Riset.xlsx', sheet_name='main')\n",
        "print(df.head(5))\n",
        "print(df.isnull().sum())\n",
        "df.describe().transpose()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8NJ79vrvtLO"
      },
      "outputs": [],
      "source": [
        "# Drop or cut the unnecessary column\n",
        "df = df.drop('Panjang Gelombang (nm)',axis=1)\n",
        "df = df.drop('N',axis=1)\n",
        "df = df.drop('P',axis=1)\n",
        "\n",
        "# create X and Y variable for training\n",
        "X = df.drop('N',axis=1)\n",
        "y = df['N']\n",
        "\n",
        "# split data to 70:30\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAa0ow-6BcJ6"
      },
      "outputs": [],
      "source": [
        "# Preprocessing Random Oversampling\n",
        "# SMOGN\n",
        "!pip install smogn\n",
        "\n",
        "# load libraries\n",
        "import smogn\n",
        "import pandas\n",
        "## conduct smogn\n",
        "df_smogn = smogn.smoter(\n",
        "    \n",
        "    ## main arguments\n",
        "    data = df,           ## pandas dataframe\n",
        "    y = 'K',          ## string ('header name')\n",
        "    k = 9,                    ## positive integer (k < n) , 9, 12\n",
        "    samp_method = 'extreme',  ## string ('balance' or 'extreme')\n",
        "\n",
        "    ## phi relevance arguments\n",
        "    rel_thres = 0.99,         ## positive real number (0 < R < 1), 0.99\n",
        "    rel_method = 'auto',      ## string ('auto' or 'manual')\n",
        "    rel_xtrm_type = 'high',   ## string ('low' or 'both' or 'high')\n",
        "    rel_coef = 0.9           ## positive real number (0 < R), 0.9, 3.6\n",
        ")\n",
        "df_smogn.to_csv('df_smogn_k.csv')\n",
        "df_smogn.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-pDxLmbHtrq"
      },
      "outputs": [],
      "source": [
        "X = df_smogn.drop('N',axis=1)\n",
        "y = df_smogn['N']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0XoDGvy_BEE"
      },
      "outputs": [],
      "source": [
        "#read df_smogn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "df = pd.read_csv('df_smogn_n.csv')\n",
        "#print(df.head(5))\n",
        "#print(df.isnull().sum())\n",
        "df = df.drop('Unnamed: 0',axis=1)\n",
        "df.describe().transpose()\n",
        "X = df.drop('N',axis=1)\n",
        "y = df['N']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwQp7ViySgOR"
      },
      "outputs": [],
      "source": [
        "#Preprocessing: Principal Component Analysis (PCA)\n",
        "# calculate matriks covariance\n",
        "features = X.T\n",
        "cov_matrix = np.cov(features)\n",
        "cov_matrix[:5]\n",
        "\n",
        "#Eigen Decomposition\n",
        "values, vectors = np.linalg.eig(cov_matrix)\n",
        "values=values.real\n",
        "vectors=vectors.real\n",
        "values[:5]\n",
        "\n",
        "explained_variances = []\n",
        "for i in range(len(values)):\n",
        " explained_variances.append(values[i] / np.sum(values))\n",
        "print(np.sum(explained_variances), '\\n', explained_variances)\n",
        "\n",
        "projected_1 = X.dot(vectors.T[0])\n",
        "projected_2 = X.dot(vectors.T[1])\n",
        "projected_3 = X.dot(vectors.T[2])\n",
        "projected_4 = X.dot(vectors.T[3])\n",
        "projected_5 = X.dot(vectors.T[4])\n",
        "projected_6 = X.dot(vectors.T[5])\n",
        "projected_7 = X.dot(vectors.T[6])\n",
        "projected_8 = X.dot(vectors.T[7])\n",
        "projected_9 = X.dot(vectors.T[8])\n",
        "projected_10 = X.dot(vectors.T[9])\n",
        "df_pca = pd.DataFrame(projected_1, columns=['PC1'])\n",
        "df_pca['PC2'] = projected_2\n",
        "df_pca['PC3'] = projected_3\n",
        "df_pca['PC4'] = projected_4\n",
        "df_pca['PC5'] = projected_5\n",
        "df_pca['PC6'] = projected_6\n",
        "df_pca['PC7'] = projected_7\n",
        "df_pca['PC8'] = projected_8\n",
        "df_pca['PC9'] = projected_9\n",
        "df_pca['PC10'] = projected_10\n",
        "df_pca['N'] = y\n",
        "df_pca.to_csv('df_pca10_n.csv')\n",
        "df_pca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htJiMALeXinN"
      },
      "outputs": [],
      "source": [
        "# Plot data to scatterplot\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(20, 10))\n",
        "sns.scatterplot(df_pca['PC1'], [0] * len(df_pca), hue=df_pca['K'], s=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPL38rKW7UFd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#Variable to save the result in .csv\n",
        "pc='raw'\n",
        "unsur='k'\n",
        "\n",
        "# choose one of these line below:\n",
        "#df_pca = pd.read_csv('df_pca10_k.csv')\n",
        "#df_pca = pd.read_excel('df_smogn_p.xlsx')\n",
        "#df_pca = pd.read_csv('df_smogn_k.csv')\n",
        "df_pca = pd.read_excel('df_raw_k.xlsx')\n",
        "\n",
        "df_pca.drop('Unnamed: 0', axis=1, inplace=True)\n",
        "df_pca.head(5)\n",
        "\n",
        "# create X and Y variable for model\n",
        "X = df_pca.drop('K',axis=1)\n",
        "y = df_pca['K']\n",
        "\n",
        "# split train and test data 80:20\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "X_train.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROdgF1vDPEGd"
      },
      "outputs": [],
      "source": [
        "#1. Random Forest\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn import metrics\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "parameters={\n",
        "    'n_estimators': [100, 200, 400, 600, 800, 1000], #200, 400, 600, 800, 1000\n",
        "    'max_depth': [None, 10, 30, 50], #10, 30, 50\n",
        "    'min_samples_split':[2, 5], #2, 5\n",
        "    'min_samples_leaf': [1, 3], #1, 3\n",
        "    'bootstrap': [False, True]\n",
        "    }\n",
        "\n",
        "rf = RandomForestRegressor()\n",
        "regr_rf = GridSearchCV(rf, parameters)\n",
        "regr_rf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = regr_rf.predict(X_test)\n",
        "\n",
        "r2=regr_rf.best_score_\n",
        "rmse=np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
        "mape=np.mean(np.abs((y_test - y_pred)/y_test))*100\n",
        "nse=1-(np.sum((y_test-y_pred)**2)/np.sum((y_test-np.mean(y_pred))**2))\n",
        "akurasi=100-np.mean(np.abs((y_test - y_pred)/y_test))*100\n",
        "best_estimator=regr_rf.best_estimator_\n",
        "best_param= regr_rf.best_params_\n",
        "\n",
        "print(\"Random Forest\")\n",
        "print('R2     :', r2)\n",
        "print('RMSE   :', rmse)\n",
        "print('MAPE   :', mape)\n",
        "print('NSE    :', nse)\n",
        "print('AKURASI:', akurasi)\n",
        "print(\"\\n Results from Grid Search \" )\n",
        "print(\"\\n The best estimator across ALL searched params:\\n\", best_estimator)\n",
        "print(\"\\n The best parameters across ALL searched params:\\n\", best_param)\n",
        "\n",
        "#Simpan semua hasil statistik dalam file csv\n",
        "import csv  \n",
        "header1 = ['R2', 'RMSE', 'MAPE', 'NSE', 'AKURASI', 'Best Estimator', 'Best Parameter']\n",
        "data1 = [r2, rmse, mape, nse, akurasi, best_estimator, best_param]\n",
        "header2 = ['y_test']\n",
        "data2 = [y_test]\n",
        "header3 = ['y_pred']\n",
        "data3 = [y_pred]\n",
        "\n",
        "model='rf'\n",
        "nama_file='data_statistik' + '_' + model + '_' + pc + '_' + unsur + '.csv'\n",
        "\n",
        "with open(nama_file, 'w', encoding='UTF8') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(header1)\n",
        "    writer.writerow(data1)\n",
        "    writer.writerow(\"\\n\")\n",
        "    writer.writerow(header2)\n",
        "    writer.writerow(data2)\n",
        "    writer.writerow(\"\\n\")\n",
        "    writer.writerow(header3)\n",
        "    writer.writerow(data3)\n",
        "\n",
        "# Plot grafik regresi\n",
        "plt.scatter(y_test,y_pred)\n",
        "plt.plot(y_test,y_test,'r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lfTanXe-Gun"
      },
      "outputs": [],
      "source": [
        "#2. Gradient Boosting\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn import metrics\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "parameters={\n",
        "    'learning_rate': [0.01, 0.1, 0.3, 0.5, 0.7, 1.0], #0.0 - inf\n",
        "    'n_estimators': [1, 3, 5, 7, 10, 15, 20, 100], #1 - inf\n",
        "    'subsample':[0.1, 0.3, 0.5, 0.7, 1], #0-1\n",
        "    'max_depth': [1, 3, 5, 7, 10, 15], #1-inf\n",
        "    }\n",
        "gb = GradientBoostingRegressor()\n",
        "regr_gb = GridSearchCV(gb, parameters)\n",
        "regr_gb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = regr_gb.predict(X_test)\n",
        "\n",
        "r2=regr_gb.best_score_\n",
        "rmse=np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
        "mape=np.mean(np.abs((y_test - y_pred)/y_test))*100\n",
        "nse=1-(np.sum((y_test-y_pred)**2)/np.sum((y_test-np.mean(y_pred))**2))\n",
        "akurasi=100-np.mean(np.abs((y_test - y_pred)/y_test))*100\n",
        "best_estimator=regr_gb.best_estimator_\n",
        "best_param= regr_gb.best_params_\n",
        "\n",
        "print(\"Gradient Boosting\")\n",
        "print('R2     :', r2)\n",
        "print('RMSE   :', rmse)\n",
        "print('MAPE   :', mape)\n",
        "print('NSE    :', nse)\n",
        "print('AKURASI:', akurasi)\n",
        "print(\"\\n Results from Grid Search \" )\n",
        "print(\"\\n The best estimator across ALL searched params:\\n\", best_estimator)\n",
        "print(\"\\n The best parameters across ALL searched params:\\n\", best_param)\n",
        "\n",
        "#Simpan semua hasil statistik dalam file csv\n",
        "import csv  \n",
        "header1 = ['R2', 'RMSE', 'MAPE', 'NSE', 'AKURASI', 'Best Estimator', 'Best Parameter']\n",
        "data1 = [r2, rmse, mape, nse, akurasi, best_estimator, best_param]\n",
        "header2 = ['y_test']\n",
        "data2 = [y_test]\n",
        "header3 = ['y_pred']\n",
        "data3 = [y_pred]\n",
        "\n",
        "model='gb'\n",
        "nama_file='data_statistik' + '_' + model + '_' + pc + '_' + unsur + '.csv'\n",
        "\n",
        "with open(nama_file, 'w', encoding='UTF8') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(header1)\n",
        "    writer.writerow(data1)\n",
        "    writer.writerow(\"\\n\")\n",
        "    writer.writerow(header2)\n",
        "    writer.writerow(data2)\n",
        "    writer.writerow(\"\\n\")\n",
        "    writer.writerow(header3)\n",
        "    writer.writerow(data3)\n",
        "\n",
        "# Plot grafik regresi\n",
        "plt.scatter(y_test,y_pred)\n",
        "plt.plot(y_test,y_test,'r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMgP6xMSsbVD"
      },
      "outputs": [],
      "source": [
        "#3 Support Vector Machine\n",
        "from sklearn.svm import SVR\n",
        "from sklearn import metrics\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "parameters={\n",
        "    'C': [1.0, 2.0, 5.0, 7.0, 10.0], #positif\n",
        "    'epsilon': [0.1, 0.3, 0.5, 0.7, 1.0],\n",
        "    'gamma':['scale', 'auto'],\n",
        "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid']\n",
        "    }\n",
        "svr = SVR()\n",
        "regr_svr = GridSearchCV(svr, parameters)\n",
        "regr_svr.fit(X_train, y_train)\n",
        "\n",
        "y_pred = regr_svr.predict(X_test)\n",
        "\n",
        "r2=regr_svr.best_score_\n",
        "rmse=np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
        "mape=np.mean(np.abs((y_test - y_pred)/y_test))*100\n",
        "nse=1-(np.sum((y_test-y_pred)**2)/np.sum((y_test-np.mean(y_pred))**2))\n",
        "akurasi=100-np.mean(np.abs((y_test - y_pred)/y_test))*100\n",
        "best_estimator=regr_svr.best_estimator_\n",
        "best_param= regr_svr.best_params_\n",
        "\n",
        "print(\"Support Vector Machine\")\n",
        "print('R2     :', r2)\n",
        "print('RMSE   :', rmse)\n",
        "print('MAPE   :', mape)\n",
        "print('NSE    :', nse)\n",
        "print('AKURASI:', akurasi)\n",
        "print(\"\\n Results from Grid Search \" )\n",
        "print(\"\\n The best estimator across ALL searched params:\\n\", best_estimator)\n",
        "print(\"\\n The best parameters across ALL searched params:\\n\", best_param)\n",
        "\n",
        "#Simpan semua hasil statistik dalam file csv\n",
        "import csv  \n",
        "header1 = ['R2', 'RMSE', 'MAPE', 'NSE', 'AKURASI', 'Best Estimator', 'Best Parameter']\n",
        "data1 = [r2, rmse, mape, nse, akurasi, best_estimator, best_param]\n",
        "header2 = ['y_test']\n",
        "data2 = [y_test]\n",
        "header3 = ['y_pred']\n",
        "data3 = [y_pred]\n",
        "\n",
        "model='svm'\n",
        "nama_file='data_statistik' + '_' + model + '_' + pc + '_' + unsur + '.csv'\n",
        "\n",
        "with open(nama_file, 'w', encoding='UTF8') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(header1)\n",
        "    writer.writerow(data1)\n",
        "    writer.writerow(\"\\n\")\n",
        "    writer.writerow(header2)\n",
        "    writer.writerow(data2)\n",
        "    writer.writerow(\"\\n\")\n",
        "    writer.writerow(header3)\n",
        "    writer.writerow(data3)\n",
        "\n",
        "# Plot grafik regresi\n",
        "plt.scatter(y_test,y_pred)\n",
        "plt.plot(y_test,y_test,'r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWhhm_n6A81l"
      },
      "outputs": [],
      "source": [
        "#4. PLS\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn import metrics\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "parameters={\n",
        "    'n_components': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 40, 60, 80, 100, 120],\n",
        "    'max_iter' : [300, 500, 1000]\n",
        "    }\n",
        "pls = PLSRegression()\n",
        "regr_pls = GridSearchCV(pls, parameters)\n",
        "regr_pls.fit(X_train, y_train)\n",
        "\n",
        "y_pred = regr_pls.predict(X_test)\n",
        "\n",
        "r2=regr_pls.best_score_\n",
        "rmse=np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
        "mape=np.mean(np.abs((y_test.ravel() - y_pred.ravel())/y_test.ravel()))*100\n",
        "nse=1-(np.sum((y_test.ravel()-y_pred.ravel())**2)/np.sum((y_test.ravel()-np.mean(y_pred.ravel()))**2))\n",
        "akurasi=100-np.mean(np.abs((y_test.ravel() - y_pred.ravel())/y_test.ravel()))*100\n",
        "best_estimator=regr_pls.best_estimator_\n",
        "best_param= regr_pls.best_params_\n",
        "\n",
        "print(\"Partial Least Square\")\n",
        "print('R2     :', r2)\n",
        "print('RMSE   :', rmse)\n",
        "print('MAPE   :', mape)\n",
        "print('NSE    :', nse)\n",
        "print('AKURASI:', akurasi)\n",
        "print(\"\\n Results from Grid Search \")\n",
        "print(\"\\n The best estimator across ALL searched params:\\n\", best_estimator)\n",
        "print(\"\\n The best parameters across ALL searched params:\\n\", best_param)\n",
        "\n",
        "#Simpan semua hasil statistik dalam file csv\n",
        "import csv  \n",
        "header1 = ['R2', 'RMSE', 'MAPE', 'NSE', 'AKURASI', 'Best Estimator', 'Best Parameter']\n",
        "data1 = [r2, rmse, mape, nse, akurasi, best_estimator, best_param]\n",
        "header2 = ['y_test']\n",
        "data2 = [y_test]\n",
        "header3 = ['y_pred']\n",
        "data3 = [y_pred]\n",
        "\n",
        "model='pls'\n",
        "nama_file='data_statistik' + '_' + model + '_' + pc + '_' + unsur + '.csv'\n",
        "\n",
        "with open(nama_file, 'w', encoding='UTF8') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(header1)\n",
        "    writer.writerow(data1)\n",
        "    writer.writerow(\"\\n\")\n",
        "    writer.writerow(header2)\n",
        "    writer.writerow(data2)\n",
        "    writer.writerow(\"\\n\")\n",
        "    writer.writerow(header3)\n",
        "    writer.writerow(data3)\n",
        "\n",
        "# Plot grafik regresi\n",
        "plt.scatter(y_test,y_pred)\n",
        "plt.plot(y_test,y_test,'r')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8RJ8s3zClHR"
      },
      "outputs": [],
      "source": [
        "#5. Deep Neural Network\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "# deeper or wider\n",
        "regr_dnn = Sequential()\n",
        "regr_dnn.add(Dense(1024, input_dim=136, kernel_initializer='normal', activation='relu'))\n",
        "regr_dnn.add(Dense(1024, kernel_initializer='normal', activation='relu'))\n",
        "regr_dnn.add(Dense(512, kernel_initializer='normal', activation='relu'))\n",
        "regr_dnn.add(Dense(512, kernel_initializer='normal', activation='relu'))\n",
        "regr_dnn.add(Dense(512, kernel_initializer='normal', activation='relu'))\n",
        "regr_dnn.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
        "regr_dnn.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
        "regr_dnn.add(Dense(256, kernel_initializer='normal', activation='relu'))\n",
        "regr_dnn.add(Dense(128, kernel_initializer='normal', activation='relu'))\n",
        "regr_dnn.add(Dense(128, kernel_initializer='normal', activation='relu'))\n",
        "regr_dnn.add(Dense(128, kernel_initializer='normal', activation='relu'))\n",
        "regr_dnn.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
        "regr_dnn.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
        "regr_dnn.add(Dense(64, kernel_initializer='normal', activation='relu'))\n",
        "regr_dnn.add(Dense(32, kernel_initializer='normal', activation='relu'))\n",
        "regr_dnn.add(Dense(32, kernel_initializer='normal', activation='relu'))\n",
        "regr_dnn.add(Dense(32, kernel_initializer='normal', activation='relu'))\n",
        "regr_dnn.add(Dense(16, kernel_initializer='normal', activation='relu'))\n",
        "regr_dnn.add(Dense(16, kernel_initializer='normal', activation='relu'))\n",
        "regr_dnn.add(Dense(16, kernel_initializer='normal', activation='relu'))\n",
        "regr_dnn.add(Dense(8, kernel_initializer='normal', activation='relu'))\n",
        "regr_dnn.add(Dense(8, kernel_initializer='normal', activation='relu'))\n",
        "regr_dnn.add(Dense(8, kernel_initializer='normal', activation='relu'))\n",
        "regr_dnn.add(Dense(4, kernel_initializer='normal', activation='relu'))\n",
        "regr_dnn.add(Dense(4, kernel_initializer='normal', activation='relu'))\n",
        "regr_dnn.add(Dense(4, kernel_initializer='normal', activation='relu'))\n",
        "regr_dnn.add(Dense(1, kernel_initializer='normal'))\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.00001)\n",
        "regr_dnn.compile(optimizer=optimizer,loss='mse')\n",
        "\n",
        "regr_dnn.fit(x=X_train,y=y_train,\n",
        "          validation_data=(X_test,y_test),\n",
        "          batch_size=8,epochs=1500)\n",
        "\n",
        "from sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score, mean_absolute_percentage_error\n",
        "y_pred = regr_dnn.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "r2=r2_score(y_test, y_pred)\n",
        "rmse=np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
        "mape=np.mean(np.abs((y_test.ravel() - y_pred.ravel())/y_test.ravel()))*100\n",
        "nse=1-(np.sum((y_test.ravel()-y_pred.ravel())**2)/np.sum((y_test.ravel()-np.mean(y_pred.ravel()))**2))\n",
        "akurasi=100-np.mean(np.abs((y_test.ravel() - y_pred.ravel())/y_test.ravel()))*100\n",
        "\n",
        "print(\"Deep Neural Network\")\n",
        "print('R2     :', r2)\n",
        "print('RMSE   :', rmse)\n",
        "print('MAPE   :', mape)\n",
        "print('NSE    :', nse)\n",
        "print('AKURASI:', akurasi)\n",
        "\n",
        "#Simpan semua hasil statistik dalam file csv\n",
        "import csv  \n",
        "header1 = ['R2', 'RMSE', 'MAPE', 'NSE', 'AKURASI']\n",
        "data1 = [r2, rmse, mape, nse, akurasi]\n",
        "header2 = ['y_test']\n",
        "data2 = [y_test]\n",
        "header3 = ['y_pred']\n",
        "data3 = [y_pred]\n",
        "\n",
        "model='dnn'\n",
        "nama_file='data_statistik' + '_' + model + '_' + pc + '_' + unsur + '.csv'\n",
        "\n",
        "with open(nama_file, 'w', encoding='UTF8') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(header1)\n",
        "    writer.writerow(data1)\n",
        "    writer.writerow(\"\\n\")\n",
        "    writer.writerow(header2)\n",
        "    writer.writerow(data2)\n",
        "    writer.writerow(\"\\n\")\n",
        "    writer.writerow(header3)\n",
        "    writer.writerow(data3)\n",
        "\n",
        "# Plot grafik regresi\n",
        "plt.scatter(y_test,y_pred)\n",
        "plt.plot(y_test,y_test,'r')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}